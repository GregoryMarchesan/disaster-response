{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd036aba71e7507b7018449e68f5e9266454771fe751f4c18b9627f7b61ab514f84",
   "display_name": "Python 3.8.2 64-bit ('venture': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\grego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\grego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\grego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "\n",
    "import sqlite3\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_filepath, model_filepath = \"../data/DisasterResponse.db\", \"../models/classifier.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreOneHotEncoding(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(database_filepath):\n",
    "    \"\"\"\n",
    "    Loads the database from a SQLite file into the script and split it in X, y and category names\n",
    "\n",
    "    Parameters:\n",
    "    database_filename (string): name of the database\n",
    "\n",
    "    Returns:\n",
    "    X (numpy array): Array containing the X values (messages)\n",
    "    y (numpy array): Array containing the y values (one hot enconding of categories)\n",
    "    category_names (list): List containing the category names in the database\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(database_filepath)\n",
    "\n",
    "    # get a cursor\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # create the test table including project_id as a primary key\n",
    "    df = pd.read_sql(\"SELECT * FROM '{}'\".format(database_filepath), con=conn)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    # genre_one_hot = pd.get_dummies(df[\"genre\"])\n",
    "\n",
    "    # X = pd.concat([df[\"message\"], genre_one_hot], axis=1).values\n",
    "    X = df[[\"message\"]].values\n",
    "    y = df.drop([\"message\", \"genre\"], axis=1).astype(int).values\n",
    "\n",
    "    category_names = df.drop([\"message\", \"genre\"], axis=1).columns\n",
    "\n",
    "    return X, y, category_names\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize the text to further process in the ML algorithm\n",
    "\n",
    "    Parameters:\n",
    "    text (string): the text from each message\n",
    "\n",
    "    Returns:\n",
    "    clean_tokens (list): The list of tokens processed\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "    Using grid search, builds the model to classify the messages\n",
    "\n",
    "    Returns:\n",
    "    model (): The trained model over the data\n",
    "    \"\"\"\n",
    "\n",
    "    text_pipeline = Pipeline([\n",
    "                              ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                              ('tfidf', TfidfTransformer())\n",
    "                            ])\n",
    "\n",
    "    genre_pipeline = Pipeline([\n",
    "                               ('genre', GenreOneHotEncoding())\n",
    "                             ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[('text_pipeline', text_pipeline, 0),\n",
    "                                                   ('num', genre_pipeline, 1)\n",
    "                                                  ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    parameters = {\n",
    "        'preprocessor__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'preprocessor__text_pipeline__vect__max_df': (0.5, 0.75, 1.0),\n",
    "        'preprocessor__text_pipeline__vect__max_features': (None, 5000, 10000),\n",
    "        'preprocessor__text_pipeline__tfidf__use_idf': (True, False),\n",
    "        'clf__n_estimators': [50, 100, 200],\n",
    "        'clf__min_samples_split': [2, 3, 4]\n",
    "    }\n",
    "\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "    return cv\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model\n",
    "\n",
    "    Parameters:\n",
    "    model (sklearn model): the trained model\n",
    "    X_test (numpy array): An array with X values to test the model\n",
    "    Y_test (numpy array): An array with Y values to test the model\n",
    "    category_names (list): Names of the categories\n",
    "\n",
    "    \"\"\"\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    for i, category in enumerate(category_names):\n",
    "        precision = precision_score(Y_test[i], Y_pred[i])\n",
    "        recall = recall_score(Y_test[i], Y_pred[i])\n",
    "        f1 = f1_score(Y_test[i], Y_pred[i])\n",
    "\n",
    "        print(\"\", category)\n",
    "        print(\"Category: {} | F1-Score: {:.2f} % | Precision: {:.2f} % | Recall: {:.2f} %\".format(category, \n",
    "                                                                                                  f1*100, \n",
    "                                                                                                  precision*100, \n",
    "                                                                                                  recall*100))\n",
    "        # break\n",
    "        # print(\"Overall Accuracy:\", accuracy)\n",
    "\n",
    "    # confusion_mat = confusion_matrix(Y_test, Y_pred, labels=category_names)\n",
    "    # accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    # print(\"Labels:\", category_names)\n",
    "    # print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    # print(\"Overall Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "def save_model(model, model_filepath):\n",
    "    \"\"\"\n",
    "    Saves the trained model into a pickle file\n",
    "\n",
    "    Parameters: \n",
    "    model (sklearn model): The trained model\n",
    "    model_filepath (string): The path to save the model\n",
    "    \"\"\"\n",
    "    joblib.dump(model, model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading data...\n    DATABASE: ../data/DisasterResponse.db\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "X, Y, category_names = load_data(database_filepath)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# print('Building model...')\n",
    "# model = build_model()\n",
    "\n",
    "# print('Training model...')\n",
    "# model.fit(X_train, Y_train)\n",
    "\n",
    "# # print('Evaluating model...')\n",
    "# evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "# # print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "# save_model(model, model_filepath)\n",
    "\n",
    "# print('Trained model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  20998.000000  20998.000000  20998.000000  20998.000000  20998.000000   \n",
       "mean       0.776741      0.170492      0.004715      0.417325      0.079246   \n",
       "std        0.434573      0.376074      0.068503      0.493129      0.270128   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "max        2.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 5             6             7             8        9   ...  \\\n",
       "count  20998.000000  20998.000000  20998.000000  20998.000000  20998.0  ...   \n",
       "mean       0.050052      0.027526      0.017954      0.033289      0.0  ...   \n",
       "std        0.218058      0.163615      0.132788      0.179394      0.0  ...   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.0  ...   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.0  ...   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.0  ...   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.0  ...   \n",
       "max        1.000000      1.000000      1.000000      1.000000      0.0  ...   \n",
       "\n",
       "                 26            27            28            29            30  \\\n",
       "count  20998.000000  20998.000000  20998.000000  20998.000000  20998.000000   \n",
       "mean       0.011144      0.043195      0.276312      0.080817      0.092342   \n",
       "std        0.104977      0.203300      0.447184      0.272561      0.289515   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      1.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 31            32            33            34            35  \n",
       "count  20998.000000  20998.000000  20998.000000  20998.000000  20998.000000  \n",
       "mean       0.011049      0.092723      0.020478      0.051767      0.193114  \n",
       "std        0.104533      0.290051      0.141632      0.221561      0.394751  \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000  \n",
       "\n",
       "[8 rows x 36 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.0</td>\n      <td>...</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n      <td>20998.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.776741</td>\n      <td>0.170492</td>\n      <td>0.004715</td>\n      <td>0.417325</td>\n      <td>0.079246</td>\n      <td>0.050052</td>\n      <td>0.027526</td>\n      <td>0.017954</td>\n      <td>0.033289</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.011144</td>\n      <td>0.043195</td>\n      <td>0.276312</td>\n      <td>0.080817</td>\n      <td>0.092342</td>\n      <td>0.011049</td>\n      <td>0.092723</td>\n      <td>0.020478</td>\n      <td>0.051767</td>\n      <td>0.193114</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.434573</td>\n      <td>0.376074</td>\n      <td>0.068503</td>\n      <td>0.493129</td>\n      <td>0.270128</td>\n      <td>0.218058</td>\n      <td>0.163615</td>\n      <td>0.132788</td>\n      <td>0.179394</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.104977</td>\n      <td>0.203300</td>\n      <td>0.447184</td>\n      <td>0.272561</td>\n      <td>0.289515</td>\n      <td>0.104533</td>\n      <td>0.290051</td>\n      <td>0.141632</td>\n      <td>0.221561</td>\n      <td>0.394751</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 36 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "pd.DataFrame(Y_train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['weather update - a cold front from cuba that could pass over haiti'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
       "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
       "       'child_alone', 'water', 'food', 'shelter', 'clothing', 'money',\n",
       "       'missing_people', 'refugees', 'death', 'other_aid',\n",
       "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
       "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
       "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
       "       'other_weather', 'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "category_names"
   ]
  }
 ]
}